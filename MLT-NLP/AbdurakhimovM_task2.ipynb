{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b87d8445-f9b5-43b5-bde7-c41842ba5c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "\n",
    "with open(\"11-0.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17bd87-9799-4cf7-ba99-9eae8a13f3bf",
   "metadata": {},
   "source": [
    "<h1>Part 1, 2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be3c0fab-1ad9-4683-9686-49f5f1e14f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abdur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import all the resources for Natural Language Processing with Python\n",
    "nltk.download(\"book\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Download NLTK resources if not already installed\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load the text\n",
    "with open(\"11-0.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Convert to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove numbers and non-alphabetic characters\n",
    "text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59ea54-5afd-4d78-b4cb-821d287cf42a",
   "metadata": {},
   "source": [
    "Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ae8793-a33c-4cc5-b5e1-eab9bb1e7f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# remove punctuation from each word\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "# lemmatizarion\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "lemmas = [wn.lemmatize(word) for word in words] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae395da9-8a1a-4598-a505-d63bf3d0fa2f",
   "metadata": {},
   "source": [
    "<h1>Part 3</h1>\n",
    "10 most important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e9c48e8-982a-4e0f-82b3-d543ee251c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurences =[i for i,x in enumerate(lemmas) if x=='chapter']\n",
    "occurences.append(len(lemmas))  # the last word (index) in the 'lemmas' variable is the end of the last chapter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a42ee0a-a785-42a1-9fb5-cf3435262b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(134, 1113),\n",
       " (1113, 2115),\n",
       " (2115, 2938),\n",
       " (2938, 4162),\n",
       " (4162, 5200),\n",
       " (5200, 6421),\n",
       " (6421, 7542),\n",
       " (7542, 8720),\n",
       " (8720, 9837),\n",
       " (9837, 10851),\n",
       " (10851, 11752),\n",
       " (11752, 14371)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices=[]\n",
    "#First 12 occurence of the word 'chapter' are just the outline of the story, thus we delete them\n",
    "chapter_points=occurences[12:] \n",
    "# building a list to contain for each chapter the indices for its start and ending.\n",
    "for i in range (len(chapter_points)-1):\n",
    "  indices.append((chapter_points[i],chapter_points[i+1]))\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e94dd531-0474-41a4-b4c5-f8530619f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'chapters' variable contains lists of lemmas for each chapter individually \n",
    "chapters = [lemmas [s:e] for s,e in indices]\n",
    "\n",
    "# delete the word 'Alice' from each paragraph (avoiding getting 'Alice' as one of the most important words)\n",
    "chapters_without_alice = [[word for word in chapter if word != 'alice'] for chapter in chapters]\n",
    "\n",
    "# for each chapter, join the tokens together \n",
    "chapters_paragraphs=[' '.join(i) for i in chapters_without_alice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08e2e8d0-1ef6-4d5b-b781-939b209f215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TF-IDF and finding top 10 words for each document\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the model\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(chapters_paragraphs).toarray()\n",
    "vocab = tfidf.vocabulary_\n",
    "reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "feature_names = tfidf.get_feature_names_out()  # Use get_feature_names_out() to access feature names\n",
    "df_tfidf = pd.DataFrame(X_tfidf, columns=feature_names)\n",
    "\n",
    "idx = X_tfidf.argsort(axis=1)  # Sorting\n",
    "tfidf_max10 = idx[:, -10:]  # Top 10\n",
    "df_tfidf['top10'] = [[reverse_vocab.get(item) for item in row] for row in tfidf_max10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5092c92c-ea81-478d-9a3c-c47bcd8e21ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1 ['bottle', 'see', 'like', 'way', 'think', 'eat', 'key', 'door', 'bat', 'little']\n",
      "Chapter 2 ['mabel', 'foot', 'said', 'dear', 'cat', 'swam', 'im', 'little', 'pool', 'mouse']\n",
      "Chapter 3 ['soon', 'bird', 'know', 'thimble', 'dry', 'lory', 'prize', 'dodo', 'said', 'mouse']\n",
      "Chapter 4 ['one', 'said', 'fan', 'bottle', 'glove', 'puppy', 'rabbit', 'window', 'little', 'bill']\n",
      "Chapter 5 ['little', 'father', 'size', 'youth', 'egg', 'im', 'pigeon', 'serpent', 'said', 'caterpillar']\n",
      "Chapter 6 ['cook', 'like', 'wow', 'duchess', 'pig', 'mad', 'baby', 'footman', 'cat', 'said']\n",
      "Chapter 7 ['know', 'draw', 'tea', 'time', 'twinkle', 'hare', 'march', 'said', 'dormouse', 'hatter']\n",
      "Chapter 8 ['executioner', 'procession', 'five', 'cat', 'soldier', 'gardener', 'king', 'hedgehog', 'said', 'queen']\n",
      "Chapter 9 ['day', 'say', 'went', 'queen', 'moral', 'duchess', 'gryphon', 'mock', 'said', 'turtle']\n",
      "Chapter 10 ['whiting', 'join', 'beautiful', 'soup', 'dance', 'lobster', 'said', 'gryphon', 'mock', 'turtle']\n",
      "Chapter 11 ['tart', 'officer', 'juror', 'queen', 'witness', 'dormouse', 'court', 'said', 'hatter', 'king']\n",
      "Chapter 12 ['copy', 'term', 'copyright', 'state', 'foundation', 'electronic', 'gutenberg', 'work', 'gutenbergtm', 'project']\n"
     ]
    }
   ],
   "source": [
    "#Printing Top 10 words for each chapter\n",
    "for (i, item) in enumerate(df_tfidf['top10'] , start=1):\n",
    "    print('Chapter', i, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d07334-032f-43fe-a989-dd84a27a70b7",
   "metadata": {},
   "source": [
    "<h1>Part 4</h1>\n",
    "Top 10 most used verbs with Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e89b7895-1dde-4edd-9ee8-1b47d5facf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the story into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Keep only the sentences that contains the word (alice) in order to find the verbs in these sentences as required.\n",
    "sentences_with_alice = [sen for sen in sentences if 'alice' in sen.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b6e7526-600f-48ab-8cf3-a3c403fbe97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagging the sentences\n",
    "sentences_tagged=[] # this list will contain each sentence tagged\n",
    "for i in sentences_with_alice: \n",
    "  # Tokenizing each sentence individually\n",
    "  wordsList = nltk.word_tokenize(i) \n",
    "  # Removing punctuation from each word\n",
    "  stripped_2 = [ w.translate(table) for w in wordsList]\n",
    "  # Removing stop words and non alphabtecis words \n",
    "  wordsList = [w.translate(table) for w in stripped_2 if not w in stop_words and w.isalpha()]\n",
    "  # Using a Tagger. Which is part-of-speech tagger or POS-tagger. \n",
    "  tagged = nltk.pos_tag(wordsList) \n",
    "  sentences_tagged.append(tagged) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e76259f-d3be-4483-899f-a0ba4e280dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the verbs in the tagged sentences\n",
    "# All the tags that are related to verb: VB, VBD, VBG, VBN, VBP, VBZ\n",
    "verb_tags=['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "verbs =[]\n",
    "# keeping only the verbs \n",
    "for sen in sentences_tagged:\n",
    "  verbs.append([verb for verb,tag in sen if tag in verb_tags])\n",
    "all_verbs=[j for i in verbs for j in i] # This variable contains all the verbs in one list\n",
    "#Finding the lemma for each verb in order to find the most frequent verb\n",
    "verbs_lemmas = [wn.lemmatize(word,'v') for word in all_verbs] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a5e942c-e9eb-4552-beb6-869141e48a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most 10 frequent verbs:\n",
      "    verb  index  counts\n",
      "0    say     40     528\n",
      "1     go     53     177\n",
      "2    get     28      97\n",
      "3  think     39      89\n",
      "4  begin     27      85\n",
      "5   look     46      81\n",
      "6   come     65      81\n",
      "7   know    107      80\n",
      "8   make     34      74\n",
      "9    see     49      65\n"
     ]
    }
   ],
   "source": [
    " # Creating a dataframe out of the list of verbs_lemmas\n",
    "df_all_words = pd.DataFrame(verbs_lemmas, columns=['verb'])\n",
    "# Grouping the verbs and find each verb frequency\n",
    "df_all_words['counts'] = df_all_words.groupby(['verb'])['verb'].transform('count')\n",
    "# Sorting the verbs by their frequencies\n",
    "df_all_words = df_all_words.sort_values(by=['counts', 'verb'], ascending=[False, True]).reset_index()\n",
    "# Finding most 10 frequent verbs\n",
    "df_words = df_all_words.groupby('verb').first().sort_values(by='counts', ascending=False).reset_index()\n",
    "print(\"Most 10 frequent verbs:\")\n",
    "print(df_words.head(10))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a0361-43a7-43ee-ac32-17834cea95cf",
   "metadata": {},
   "source": [
    "Based on the analysis of the text, we have identified the ten most frequent verbs in the document. These verbs and their respective counts are as follows:\r\n",
    "\r\n",
    "\"say\" with 528 occurrences.\r\n",
    "\"go\" with 177 occurrences.\r\n",
    "\"get\" with 97 occurrences.\r\n",
    "\"think\" with 89 occurrences.\r\n",
    "\"begin\" with 85 occurrences.\r\n",
    "\"look\" with 81 occurrences.\r\n",
    "\"come\" with 81 occurrences.\r\n",
    "\"know\" with 80 occurrences.\r\n",
    "\"make\" with 74 occurrences.\r\n",
    "\"see\" with 65 occurrences.\r\n",
    "These results provide insights into the verbs that are most frequently used in the text. Verbs like \"say,\" \"go,\" and \"get\" are the most common, indicating their significant presence in the narrative. Additionally, verbs like \"think,\" \"begin,\" and \"look\" are also frequently used, contributing to the overall understanding of the actions and events described in the text.\r\n",
    "\r\n",
    "It's important to note that this analysis can help readers and researchers gain a better understanding of the text's themes, character actions, and storytelling style. The frequency of verbs provides valuable information about the narrative's dynamics and key elements.\r\n",
    "\r\n",
    "In conclusion, this analysis of the most frequent verbs in the text sheds light on the language and narrative style employed by the author, contributing to a deeper comprehension of the text's content and themes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
