{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMb9ud3x01PbZ1kP5WDB9jW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrSimple07/MachineLearning_ITMO/blob/main/machine_learning_exam_1st_semester.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Technologies (September 2023) EXAM\n"
      ],
      "metadata": {
        "id": "R0Z3Q4YaeADC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Supervised learning (an overview of the tasks and algorithms).\n",
        "2. Unsupervised learning (an overview of the tasks and algorithms).\n",
        "3. Machine learning and Bayes theorem. Prior and posterior distribution.\n",
        "4. Error decomposition. Bias and variance tradeoff.\n",
        "5. Linear regression model and Logistic regression.\n",
        "6. The method of the k-nearest neighbors. Support vector machines. Kernel trick.\n",
        "7. Decision trees and principles of its construction.\n",
        "8. Types of features. Feature selection approaches. One-hot-encoding.\n",
        "9. Data visualization and dimension-reduction algorithms: PCA and t-SNE.\n",
        "10. Classification metrics. Accuracy, precision, recall, F1-score, log-loss, ROC-AUC. Types of errors, confusion matrix. Metrics of accuracy for regression models.\n",
        "11. Construction of Ensembles of algorithms. Random Forrest.\n",
        "12. Clustering algorithms. K-means and DBSCAN. Estimation of clustering quality.\n",
        "13. Multilayer perceptron. Activation functions and loss functions in neural networks. Parameters and hyperparameters.\n",
        "14. Training of the deep neural network as an optimization problem. Gradient descent, stochastic gradient descent, Momentum, RMSProp and Adam algorithms.\n",
        "15. Deep multi-layer neural networks. Backpropagation algorithm. The problem of vanishing and exploding gradients and the methods of its solution.\n",
        "16. Datasets: train, test, validation (dev) sets. Cross-validation. Monitoring the learning process. Overfitting.\n",
        "17. Convolutional neural networks (CNNs): convolution, pooling, padding, feature maps, low-level and high-level features.\n",
        "18. Transfer learning approach. An overview of modern CNN architectures and open-source datasets. Advantages and disadvantages of modern CNNs.\n",
        "19. Natural language processing. Bag of words approach. TF-IDF method. Stemming and lemmatization. Stop words.\n",
        "20. Word embeddings. Skip-gram model. Word2vec, Glove, BERT.\n",
        "21. Sequence analysis tasks. Simple recurrent neural network architecture.\n",
        "22. LSTM and GRU cells. Memory in neural networks.\n"
      ],
      "metadata": {
        "id": "PgOoxY28eNWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Supervised Learning\n",
        "\n",
        "Supervised learning is a type of machine learning where the model learns from **examples with correct answers** (called labels). It‚Äôs like learning with a teacher: you see input data and are told the correct output.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç How It Works\n",
        "\n",
        "1. **Input Data (X)**: Features (like size, color, age, etc.)\n",
        "2. **Output Labels (Y)**: Correct answers (like price, category, etc.)\n",
        "3. **Model**: A mathematical function learns the relationship between X and Y.\n",
        "4. **Training**: The model adjusts itself to reduce mistakes.\n",
        "5. **Prediction**: After training, it can guess Y for new X.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Goals (Tasks)\n",
        "\n",
        "### 1. Classification\n",
        "- Predicts a **category or class** (discrete value).\n",
        "- Example: Email ‚Üí Spam or Not Spam\n",
        "\n",
        "### 2. Regression\n",
        "- Predicts a **number** (continuous value).\n",
        "- Example: House Features ‚Üí Price\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Common Algorithms\n",
        "\n",
        "| Type          | Algorithms                            |\n",
        "|---------------|----------------------------------------|\n",
        "| Classification| Logistic Regression, SVM, KNN, Trees   |\n",
        "| Regression    | Linear Regression, Decision Trees      |\n",
        "| Both          | Neural Networks, Random Forest, XGBoost|\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Process\n",
        "\n",
        "1. **Collect Data** (with labels)\n",
        "2. **Split** into training/test sets\n",
        "3. **Train** the model on training set\n",
        "4. **Test** on unseen data\n",
        "5. **Evaluate** using metrics (accuracy, MAE, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Pros\n",
        "- Predictable and measurable\n",
        "- Works well with enough labeled data\n",
        "\n",
        "## ‚ùå Cons\n",
        "- Needs labeled data (can be expensive to get)\n",
        "- May not work well with noisy or biased data\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Summary\n",
        "> Supervised learning teaches machines to map inputs to outputs using example data. It's used for tasks like spam detection, price prediction, medical diagnosis, and more.\n",
        "\n"
      ],
      "metadata": {
        "id": "qWhfjvTdkYrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 ü§ñ Unsupervised Learning ‚Äî Explained Simply\n",
        "\n",
        "Unsupervised learning is a type of machine learning where the model **finds patterns** in data **without any labels**. It's like exploring a new place with no guide ‚Äî the model figures out the structure on its own.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç How It Works\n",
        "\n",
        "1. **Input Data (X)**: Only features, no correct answers\n",
        "2. **Model**: Learns hidden patterns, groupings, or structures in the data\n",
        "3. **Goal**: Discover insights or simplify the data\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Goals (Tasks)\n",
        "\n",
        "### 1. Clustering\n",
        "- Group similar items together\n",
        "- Example: Group customers by shopping behavior\n",
        "\n",
        "### 2. Dimensionality Reduction\n",
        "- Reduce the number of features while keeping key information\n",
        "- Example: Visualizing high-dimensional data in 2D\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Common Algorithms\n",
        "\n",
        "| Task                   | Algorithms                            |\n",
        "|------------------------|----------------------------------------|\n",
        "| Clustering             | K-Means, DBSCAN, Hierarchical Clustering |\n",
        "| Dimensionality Reduction | PCA, t-SNE, Autoencoders               |\n",
        "| Association Rule Mining | Apriori, Eclat                        |\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Process\n",
        "\n",
        "1. **Collect Data** (no labels needed)\n",
        "2. **Preprocess** (clean, scale, etc.)\n",
        "3. **Train** the model to find patterns\n",
        "4. **Analyze** output (e.g. clusters, components)\n",
        "5. **Interpret** results for decision-making\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Pros\n",
        "- No need for labeled data\n",
        "- Good for exploring unknown patterns\n",
        "\n",
        "## ‚ùå Cons\n",
        "- Harder to evaluate results\n",
        "- Can find meaningless patterns if not used carefully\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Summary\n",
        "> Unsupervised learning helps machines understand the hidden structure of data ‚Äî great for clustering, visualization, and data exploration when you don't have labels.\n"
      ],
      "metadata": {
        "id": "3V00xriQuKIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 # üß† Machine Learning & Bayes' Theorem\n",
        "\n",
        "Bayes‚Äô Theorem is a way to **update our beliefs** (probabilities) based on **new evidence**. It‚Äôs used in **probabilistic models** in Machine Learning.\n",
        "\n",
        "---\n",
        "\n",
        "## üìò Formula\n",
        "\n",
        "**Bayes' Theorem:**\n",
        "$$\n",
        "\\[\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "\\]\n",
        "$$\n",
        "---\n",
        "\n",
        "## üß© Key Terms\n",
        "\n",
        "- **Prior (P(A))**: What we believe before seeing data\n",
        "- **Likelihood (P(B|A))**: Probability of seeing data if A is true\n",
        "- **Posterior (P(A|B))**: Updated belief after seeing data\n",
        "- **Evidence (P(B))**: Total probability of the data\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ In ML\n",
        "\n",
        "Used in:\n",
        "- **Naive Bayes classifier**\n",
        "- **Bayesian inference**\n",
        "- **Bayesian neural networks**\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Example (Spam Detection)\n",
        "\n",
        "- **Prior**: Chance an email is spam (e.g., 20%)\n",
        "- **Likelihood**: If \"free\" appears, how likely is spam?\n",
        "- **Posterior**: New spam probability after seeing \"free\"\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Summary\n",
        "\n",
        "> Bayes‚Äô theorem lets ML models **learn from evidence** by updating probabilities. It‚Äôs the core of **Bayesian thinking** in AI.\n"
      ],
      "metadata": {
        "id": "lXdAlUnLud_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 # üéØ Error Decomposition: Bias-Variance Tradeoff\n",
        "\n",
        "In Machine Learning, total prediction error can be split into three parts: **bias**, **variance**, and **irreducible error**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Total Error = Bias¬≤ + Variance + Irreducible Error\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Bias\n",
        "- **What**: Error from wrong assumptions in the model.\n",
        "- **High Bias**: Model is too simple ‚Üí underfitting.\n",
        "- **Example**: Linear model for complex patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Variance\n",
        "- **What**: Error from model sensitivity to training data.\n",
        "- **High Variance**: Model is too complex ‚Üí overfitting.\n",
        "- **Example**: Model learns noise in training data.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öñÔ∏è Tradeoff\n",
        "- **Goal**: Find the balance between bias and variance.\n",
        "- **Simple model** ‚Üí low variance, high bias.\n",
        "- **Complex model** ‚Üí low bias, high variance.\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Visualization\n",
        "\n",
        "- Underfitting: üéØ misses target completely ‚Üí high bias\n",
        "- Overfitting: üéØ hits different spots every time ‚Üí high variance\n",
        "- Good fit: üéØ hits near the center consistently\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary\n",
        "\n",
        "> Bias-Variance tradeoff explains **why models make errors** and helps us choose **right model complexity**.\n"
      ],
      "metadata": {
        "id": "_lPvlkZFvjWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 üìâ Linear Regression vs üîê Logistic Regression\n",
        "\n",
        "---\n",
        "\n",
        "## üî¢ Linear Regression\n",
        "\n",
        "- **Goal**: Predict a continuous value (e.g., price, weight).\n",
        "- **Formula**:  \n",
        "  `y = w‚ÇÄ + w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô`  \n",
        "  (a straight line or hyperplane)\n",
        "\n",
        "- **How it works**:  \n",
        "  Finds the best line that fits the training data by minimizing the difference between predicted and actual values (using **Mean Squared Error**).\n",
        "\n",
        "- **Example**:  \n",
        "  Predict house price based on area and number of rooms.\n",
        "\n",
        "---\n",
        "\n",
        "## üö¶ Logistic Regression\n",
        "\n",
        "- **Goal**: Predict probability for **classification** (e.g., spam or not spam).\n",
        "- **Formula**:  \n",
        "  `P(y=1|x) = 1 / (1 + e^-(w‚ÇÄ + w‚ÇÅx‚ÇÅ + ... + w‚Çôx‚Çô))`  \n",
        "  (sigmoid function)\n",
        "\n",
        "- **How it works**:  \n",
        "  Outputs a number between 0 and 1 ‚Üí interprets as probability ‚Üí sets threshold (e.g., 0.5) for classification.\n",
        "\n",
        "- **Example**:  \n",
        "  Predict if a student passes (yes/no) based on study hours.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary Table\n",
        "\n",
        "| Feature               | Linear Regression         | Logistic Regression          |\n",
        "|----------------------|---------------------------|------------------------------|\n",
        "| Output Type          | Continuous number          | Probability / Class (0 or 1) |\n",
        "| Used For             | Regression problems        | Binary classification        |\n",
        "| Activation Function  | None (just linear)         | Sigmoid                      |\n",
        "| Loss Function        | Mean Squared Error (MSE)   | Log Loss (Cross Entropy)     |\n"
      ],
      "metadata": {
        "id": "q3-GDbB2vlP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 # ü§ñ k-NN, SVM, and Kernel Trick\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ k-Nearest Neighbors (k-NN)\n",
        "\n",
        "- **What it is**: A lazy, simple algorithm that classifies a point by looking at its **k closest neighbors**.\n",
        "- **How it works**:\n",
        "  1. Choose `k` (e.g., 3).\n",
        "  2. Measure distance (usually Euclidean) to all points in training set.\n",
        "  3. Find the `k` nearest ones.\n",
        "  4. Assign the most common label among them.\n",
        "\n",
        "- **Use**: Classification or regression.\n",
        "\n",
        "- **Example**: To classify a fruit by shape/size, check the 3 most similar fruits already labeled.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è Support Vector Machines (SVM)\n",
        "\n",
        "- **What it is**: A powerful classifier that finds the **best boundary (hyperplane)** that separates classes.\n",
        "- **Goal**: Maximize the **margin** between two classes.\n",
        "\n",
        "- **How it works**:\n",
        "  - Finds a hyperplane that best separates the data.\n",
        "  - Only the closest points (called **support vectors**) affect the boundary.\n",
        "\n",
        "- **Use**: Works well for high-dimensional data and text classification.\n",
        "\n",
        "- **Example**: Email spam detection, image classification.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Kernel Trick\n",
        "\n",
        "- **Problem**: Some data is **not linearly separable** (can‚Äôt draw a straight line to split).\n",
        "- **Solution**: Use a **kernel** function to map data into a **higher dimension** where it becomes separable.\n",
        "\n",
        "- **Popular Kernels**:\n",
        "  - Polynomial\n",
        "  - Radial Basis Function (RBF)\n",
        "\n",
        "- **Key idea**: Do math to simulate higher dimensions **without actually computing them** (saves time and memory).\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary\n",
        "\n",
        "| Method       | Type         | Strength                          | Weakness                      |\n",
        "|--------------|--------------|-----------------------------------|-------------------------------|\n",
        "| k-NN         | Lazy learner | Easy to understand and use        | Slow for large datasets       |\n",
        "| SVM          | Hard margin  | Works well with high-dimensional data | Not good for very large datasets |\n",
        "| Kernel Trick |\n"
      ],
      "metadata": {
        "id": "rZW09Owsv5gF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 # üå≥ Decision Trees and How They Work\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ What is a Decision Tree?\n",
        "\n",
        "A **decision tree** is a flowchart-like model used for **classification** or **regression**.  \n",
        "It splits data into **branches** based on features, leading to a final **decision (leaf)**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß± How It Works\n",
        "\n",
        "1. **Start** at the root (all data).\n",
        "2. **Choose the best feature** to split the data (based on criteria like Gini or Entropy).\n",
        "3. **Split** the data into groups.\n",
        "4. Repeat for each branch until:\n",
        "   - All data in a node is pure (same class), or\n",
        "   - Max depth or min samples is reached.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Splitting Criteria\n",
        "\n",
        "- **Gini Impurity** (used in CART): Measures how mixed the labels are.\n",
        "- **Entropy & Information Gain** (used in ID3/C4.5):\n",
        "  - Entropy: Disorder in the data.\n",
        "  - Info Gain: How much disorder is reduced by the split.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Example\n",
        "\n",
        "If predicting if a person will buy a phone:\n",
        "- Root: Age\n",
        "  - Age < 30 ‚Üí Student?\n",
        "    - Yes ‚Üí Buys\n",
        "    - No ‚Üí Doesn‚Äôt buy\n",
        "  - Age > 30 ‚Üí Income?\n",
        "    - High ‚Üí Buys\n",
        "    - Low ‚Üí Doesn‚Äôt buy\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è Pros and Cons\n",
        "\n",
        "| Pros                        | Cons                             |\n",
        "|-----------------------------|----------------------------------|\n",
        "| Easy to understand and use  | Can overfit (too specific)       |\n",
        "| Works with numeric/categorical data | Not great with noisy data   |\n",
        "| No need to normalize data   | Instable to small changes        |\n",
        "\n",
        "---\n",
        "\n",
        "## üå≤ Final Tip\n",
        "\n",
        "For better performance, use **Random Forests** (many trees combined) to reduce overfitting.\n"
      ],
      "metadata": {
        "id": "IPccMyDXwDnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 # üî¢ Features in Machine Learning\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Types of Features\n",
        "\n",
        "1. **Numerical (Continuous)**  \n",
        "   - Example: Age, Salary  \n",
        "   - Can be any number.\n",
        "\n",
        "2. **Categorical (Discrete)**  \n",
        "   - Example: Gender, Country  \n",
        "   - Stored as labels or strings.\n",
        "\n",
        "3. **Ordinal**  \n",
        "   - Ordered categories.  \n",
        "   - Example: Education level (High School < Bachelor < Master).\n",
        "\n",
        "4. **Boolean/Binary**  \n",
        "   - True/False, 0/1  \n",
        "   - Example: IsStudent = Yes/No\n",
        "\n",
        "5. **Text / Time / Image / Audio**  \n",
        "   - Require special preprocessing.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Feature Selection Approaches\n",
        "\n",
        "Used to pick only the **most useful features** (avoid noise and reduce overfitting):\n",
        "\n",
        "1. **Filter Methods**  \n",
        "   - Use statistics like correlation, chi-squared.  \n",
        "   - Fast but ignore model performance.\n",
        "\n",
        "2. **Wrapper Methods**  \n",
        "   - Use model performance to evaluate combinations (e.g., forward/backward selection).  \n",
        "   - More accurate but slower.\n",
        "\n",
        "3. **Embedded Methods**  \n",
        "   - Feature selection built into the model (e.g., Lasso Regression, Decision Trees).\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ One-Hot Encoding\n",
        "\n",
        "A method to convert **categorical features** into numbers:\n",
        "\n",
        "- Creates a new column for each unique value.\n",
        "- Puts `1` where it matches, `0` elsewhere.\n",
        "\n",
        "### Example:\n",
        "\n",
        "| Country   | ‚Üí One-Hot |\n",
        "|-----------|-----------|\n",
        "| France    | [1, 0, 0] |\n",
        "| Germany   | [0, 1, 0] |\n",
        "| Spain     | [0, 0, 1] |\n",
        "\n",
        "Used to make **categorical data usable** by machine learning models.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary\n",
        "\n",
        "- Different feature types need different handling.\n",
        "- Feature selection improves performance.\n",
        "- One-hot encoding transforms categories into machine-friendly format.\n"
      ],
      "metadata": {
        "id": "vCpvHWbaw7zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 # üìä Data Visualization & Dimensionality Reduction\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Data Visualization\n",
        "\n",
        "**Data visualization** is the process of representing data graphically. It helps in understanding the structure, patterns, and relationships in the data. Common visualizations:\n",
        "\n",
        "- **Histograms**: Distribution of a single variable.\n",
        "- **Scatter Plots**: Relationship between two continuous variables.\n",
        "- **Bar Charts**: Comparison of categorical data.\n",
        "- **Heatmaps**: Correlation or intensity of data across dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "## üîΩ Principal Component Analysis (PCA)\n",
        "\n",
        "PCA is a **dimensionality reduction** technique that transforms data into fewer dimensions while retaining as much information as possible.\n",
        "\n",
        "### How It Works:\n",
        "- Identifies the **principal components** (directions in which data varies the most).\n",
        "- Projects the data onto these components.\n",
        "- Reduces dimensions by keeping only the most important components.\n",
        "\n",
        "### Example:\n",
        "For a dataset with many variables (features), PCA reduces it to 2-3 main features that still represent the data well, making it easier to visualize or analyze.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
        "\n",
        "t-SNE is a **non-linear dimensionality reduction** technique designed for **visualizing high-dimensional data** in 2 or 3 dimensions. It focuses on preserving local structures.\n",
        "\n",
        "### How It Works:\n",
        "- It minimizes the difference between pairwise similarities in high and low-dimensional space.\n",
        "- Helps reveal patterns like clusters or groups in data.\n",
        "\n",
        "### When to Use:\n",
        "- Best used for visualizing data like images, text embeddings, or anything with high-dimensional features.\n",
        "\n",
        "### Example:\n",
        "It‚Äôs often used to visualize the clustering of data, like when you apply it to a neural network's activations or a word embedding.\n",
        "\n",
        "---\n",
        "\n",
        "## üìâ Summary\n",
        "\n",
        "- **Data Visualization** helps to understand data visually.\n",
        "- **PCA** is a linear method that reduces the dimensionality while retaining the variance.\n",
        "- **t-SNE** is a non-linear method focused on preserving local relationships for better visualization.\n",
        "\n"
      ],
      "metadata": {
        "id": "OGTShPWBxAok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 # üßÆ Classification Metrics & Evaluation\n",
        "\n",
        "---\n",
        "\n",
        "## üî¢ Key Classification Metrics\n",
        "\n",
        "### 1. **Accuracy**\n",
        "- **Definition**: The proportion of correctly predicted instances out of the total instances.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{Accuracy} = \\frac{\\text{True Positives + True Negatives}}{\\text{Total Instances}}\n",
        "  $$\n",
        "- **Use Case**: Good for balanced datasets but not ideal for imbalanced datasets.\n",
        "\n",
        "### 2. **Precision**\n",
        "- **Definition**: The proportion of true positive predictions out of all positive predictions made.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\n",
        "  $$\n",
        "- **Use Case**: Useful when false positives are more costly (e.g., email spam detection).\n",
        "\n",
        "### 3. **Recall (Sensitivity or True Positive Rate)**\n",
        "- **Definition**: The proportion of true positive predictions out of all actual positives.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\n",
        "  $$\n",
        "- **Use Case**: Useful when missing a positive instance is costly (e.g., detecting diseases).\n",
        "\n",
        "### 4. **F1-Score**\n",
        "- **Definition**: The harmonic mean of precision and recall, balancing both metrics.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}\n",
        "  $$\n",
        "- **Use Case**: Useful when both false positives and false negatives are important and you need a balanced metric.\n",
        "\n",
        "### 5. **Log-Loss (Logarithmic Loss)**\n",
        "- **Definition**: Measures the accuracy of a classifier by penalizing incorrect classifications, especially when confident about wrong predictions.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{Log-Loss} = - \\frac{1}{N} \\sum_{i=1}^N y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)\n",
        "  $$\n",
        "  where \\( y_i \\) is the true label and \\( p_i \\) is the predicted probability.\n",
        "- **Use Case**: Especially useful for probabilistic classifiers (e.g., logistic regression, neural networks).\n",
        "\n",
        "### 6. **ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**\n",
        "- **Definition**: A performance measurement for classification problems at various thresholds settings.\n",
        "- **Use Case**: Evaluates the trade-off between true positive rate and false positive rate. The higher the AUC, the better the model.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Confusion Matrix\n",
        "\n",
        "A **Confusion Matrix** is a table that describes the performance of a classification model by comparing predicted labels with true labels. It contains four values:\n",
        "\n",
        "|               | Predicted Positive | Predicted Negative |\n",
        "|---------------|--------------------|--------------------|\n",
        "| **Actual Positive**   | True Positive (TP)  | False Negative (FN)  |\n",
        "| **Actual Negative**   | False Positive (FP) | True Negative (TN)  |\n",
        "\n",
        "- **True Positive (TP)**: Correctly predicted positive class.\n",
        "- **False Positive (FP)**: Incorrectly predicted as positive.\n",
        "- **True Negative (TN)**: Correctly predicted negative class.\n",
        "- **False Negative (FN)**: Incorrectly predicted as negative.\n",
        "\n",
        "### **Types of Errors**\n",
        "- **False Positive (Type I error)**: Incorrectly predicting a positive when it‚Äôs actually negative.\n",
        "- **False Negative (Type II error)**: Incorrectly predicting a negative when it‚Äôs actually positive.\n",
        "\n",
        "---\n",
        "\n",
        "## üìè Metrics for Regression Models\n",
        "\n",
        "### 1. **Mean Absolute Error (MAE)**\n",
        "- **Definition**: The average of the absolute errors between predicted and actual values.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{MAE} = \\frac{1}{N} \\sum_{i=1}^N |y_i - \\hat{y}_i|\n",
        "  $$\n",
        "\n",
        "### 2. **Mean Squared Error (MSE)**\n",
        "- **Definition**: The average of the squared differences between predicted and actual values. More sensitive to large errors.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n",
        "  $$\n",
        "\n",
        "### 3. **Root Mean Squared Error (RMSE)**\n",
        "- **Definition**: The square root of MSE. Gives an idea of the magnitude of error in the same units as the original data.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{RMSE} = \\sqrt{\\text{MSE}}\n",
        "  $$\n",
        "\n",
        "### 4. **R-squared (R¬≤)**\n",
        "- **Definition**: Measures how well the model explains the variance of the target variable. Ranges from 0 to 1, with higher values indicating a better fit.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## üìâ Summary\n",
        "\n",
        "- **Classification Metrics**: Help assess the performance of classification models with metrics like accuracy, precision, recall, and F1-score.\n",
        "- **Confusion Matrix**: A useful table to visualize the performance of a classification model and the types of errors made.\n",
        "- **Regression Metrics**: Used to evaluate regression models with metrics like MAE, MSE, RMSE, and R¬≤.\n"
      ],
      "metadata": {
        "id": "lTs8LvI5xPQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Supervised learning (an overview of the tasks and algorithms).\n",
        "2. Unsupervised learning (an overview of the tasks and algorithms).\n",
        "3. Machine learning and Bayes theorem. Prior and posterior distribution.\n",
        "4. Error decomposition. Bias and variance tradeoff.\n",
        "5. Linear regression model and Logistic regression.\n",
        "6. The method of the k-nearest neighbors. Support vector machines. Kernel trick.\n",
        "7. Decision trees and principles of its construction.\n",
        "8. Types of features. Feature selection approaches. One-hot-encoding.\n",
        "9. Data visualization and dimension-reduction algorithms: PCA and t-SNE.\n",
        "10. Classification metrics. Accuracy, precision, recall, F1-score, log-loss, ROC-AUC. Types of errors, confusion matrix. Metrics of accuracy for regression models.\n",
        "11. Construction of Ensembles of algorithms. Random Forrest.\n",
        "12. Clustering algorithms. K-means and DBSCAN. Estimation of clustering quality.\n",
        "13. Multilayer perceptron. Activation functions and loss functions in neural networks. Parameters and hyperparameters.\n",
        "14. Training of the deep neural network as an optimization problem. Gradient descent, stochastic gradient descent, Momentum, RMSProp and Adam algorithms.\n",
        "15. Deep multi-layer neural networks. Backpropagation algorithm. The problem of vanishing and exploding gradients and the methods of its solution.\n",
        "16. Datasets: train, test, validation (dev) sets. Cross-validation. Monitoring the learning process. Overfitting.\n",
        "17. Convolutional neural networks (CNNs): convolution, pooling, padding, feature maps, low-level and high-level features.\n",
        "18. Transfer learning approach. An overview of modern CNN architectures and open-source datasets. Advantages and disadvantages of modern CNNs.\n",
        "19. Natural language processing. Bag of words approach. TF-IDF method. Stemming and lemmatization. Stop words.\n",
        "20. Word embeddings. Skip-gram model. Word2vec, Glove, BERT.\n",
        "21. Sequence analysis tasks. Simple recurrent neural network architecture.\n",
        "22. LSTM and GRU cells. Memory in neural networks.\n"
      ],
      "metadata": {
        "id": "eUYShT54uJj6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oQkqw3sSeLO2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}