{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMW5fTbfqOBOg0IPzteF/3X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrSimple07/MachineLearning_ITMO/blob/main/machine_learning_exam_1st_semester.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Technologies (September 2023) EXAM\n"
      ],
      "metadata": {
        "id": "R0Z3Q4YaeADC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Supervised learning (an overview of the tasks and algorithms).\n",
        "2. Unsupervised learning (an overview of the tasks and algorithms).\n",
        "3. Machine learning and Bayes theorem. Prior and posterior distribution.\n",
        "4. Error decomposition. Bias and variance tradeoff.\n",
        "5. Linear regression model and Logistic regression.\n",
        "6. The method of the k-nearest neighbors. Support vector machines. Kernel trick.\n",
        "7. Decision trees and principles of its construction.\n",
        "8. Types of features. Feature selection approaches. One-hot-encoding.\n",
        "9. Data visualization and dimension-reduction algorithms: PCA and t-SNE.\n",
        "10. Classification metrics. Accuracy, precision, recall, F1-score, log-loss, ROC-AUC. Types of errors, confusion matrix. Metrics of accuracy for regression models.\n",
        "11. Construction of Ensembles of algorithms. Random Forrest.\n",
        "12. Clustering algorithms. K-means and DBSCAN. Estimation of clustering quality.\n",
        "13. Multilayer perceptron. Activation functions and loss functions in neural networks. Parameters and hyperparameters.\n",
        "14. Training of the deep neural network as an optimization problem. Gradient descent, stochastic gradient descent, Momentum, RMSProp and Adam algorithms.\n",
        "15. Deep multi-layer neural networks. Backpropagation algorithm. The problem of vanishing and exploding gradients and the methods of its solution.\n",
        "16. Datasets: train, test, validation (dev) sets. Cross-validation. Monitoring the learning process. Overfitting.\n",
        "17. Convolutional neural networks (CNNs): convolution, pooling, padding, feature maps, low-level and high-level features.\n",
        "18. Transfer learning approach. An overview of modern CNN architectures and open-source datasets. Advantages and disadvantages of modern CNNs.\n",
        "19. Natural language processing. Bag of words approach. TF-IDF method. Stemming and lemmatization. Stop words.\n",
        "20. Word embeddings. Skip-gram model. Word2vec, Glove, BERT.\n",
        "21. Sequence analysis tasks. Simple recurrent neural network architecture.\n",
        "22. LSTM and GRU cells. Memory in neural networks.\n"
      ],
      "metadata": {
        "id": "PgOoxY28eNWv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oQkqw3sSeLO2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}