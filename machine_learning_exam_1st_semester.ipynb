{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOGrWCTu85efn8c+PVz9UMZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrSimple07/MachineLearning_ITMO/blob/main/machine_learning_exam_1st_semester.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Technologies (September 2023) EXAM\n"
      ],
      "metadata": {
        "id": "R0Z3Q4YaeADC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Supervised learning (an overview of the tasks and algorithms).\n",
        "2. Unsupervised learning (an overview of the tasks and algorithms).\n",
        "3. Machine learning and Bayes theorem. Prior and posterior distribution.\n",
        "4. Error decomposition. Bias and variance tradeoff.\n",
        "5. Linear regression model and Logistic regression.\n",
        "6. The method of the k-nearest neighbors. Support vector machines. Kernel trick.\n",
        "7. Decision trees and principles of its construction.\n",
        "8. Types of features. Feature selection approaches. One-hot-encoding.\n",
        "9. Data visualization and dimension-reduction algorithms: PCA and t-SNE.\n",
        "10. Classification metrics. Accuracy, precision, recall, F1-score, log-loss, ROC-AUC. Types of errors, confusion matrix. Metrics of accuracy for regression models.\n",
        "11. Construction of Ensembles of algorithms. Random Forrest.\n",
        "12. Clustering algorithms. K-means and DBSCAN. Estimation of clustering quality.\n",
        "13. Multilayer perceptron. Activation functions and loss functions in neural networks. Parameters and hyperparameters.\n",
        "14. Training of the deep neural network as an optimization problem. Gradient descent, stochastic gradient descent, Momentum, RMSProp and Adam algorithms.\n",
        "15. Deep multi-layer neural networks. Backpropagation algorithm. The problem of vanishing and exploding gradients and the methods of its solution.\n",
        "16. Datasets: train, test, validation (dev) sets. Cross-validation. Monitoring the learning process. Overfitting.\n",
        "17. Convolutional neural networks (CNNs): convolution, pooling, padding, feature maps, low-level and high-level features.\n",
        "18. Transfer learning approach. An overview of modern CNN architectures and open-source datasets. Advantages and disadvantages of modern CNNs.\n",
        "19. Natural language processing. Bag of words approach. TF-IDF method. Stemming and lemmatization. Stop words.\n",
        "20. Word embeddings. Skip-gram model. Word2vec, Glove, BERT.\n",
        "21. Sequence analysis tasks. Simple recurrent neural network architecture.\n",
        "22. LSTM and GRU cells. Memory in neural networks.\n"
      ],
      "metadata": {
        "id": "PgOoxY28eNWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Supervised Learning\n",
        "\n",
        "Supervised learning is a type of machine learning where the model learns from **examples with correct answers** (called labels). It‚Äôs like learning with a teacher: you see input data and are told the correct output.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç How It Works\n",
        "\n",
        "1. **Input Data (X)**: Features (like size, color, age, etc.)\n",
        "2. **Output Labels (Y)**: Correct answers (like price, category, etc.)\n",
        "3. **Model**: A mathematical function learns the relationship between X and Y.\n",
        "4. **Training**: The model adjusts itself to reduce mistakes.\n",
        "5. **Prediction**: After training, it can guess Y for new X.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Goals (Tasks)\n",
        "\n",
        "### 1. Classification\n",
        "- Predicts a **category or class** (discrete value).\n",
        "- Example: Email ‚Üí Spam or Not Spam\n",
        "\n",
        "### 2. Regression\n",
        "- Predicts a **number** (continuous value).\n",
        "- Example: House Features ‚Üí Price\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Common Algorithms\n",
        "\n",
        "| Type          | Algorithms                            |\n",
        "|---------------|----------------------------------------|\n",
        "| Classification| Logistic Regression, SVM, KNN, Trees   |\n",
        "| Regression    | Linear Regression, Decision Trees      |\n",
        "| Both          | Neural Networks, Random Forest, XGBoost|\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Process\n",
        "\n",
        "1. **Collect Data** (with labels)\n",
        "2. **Split** into training/test sets\n",
        "3. **Train** the model on training set\n",
        "4. **Test** on unseen data\n",
        "5. **Evaluate** using metrics (accuracy, MAE, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Pros\n",
        "- Predictable and measurable\n",
        "- Works well with enough labeled data\n",
        "\n",
        "## ‚ùå Cons\n",
        "- Needs labeled data (can be expensive to get)\n",
        "- May not work well with noisy or biased data\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Summary\n",
        "> Supervised learning teaches machines to map inputs to outputs using example data. It's used for tasks like spam detection, price prediction, medical diagnosis, and more.\n",
        "\n"
      ],
      "metadata": {
        "id": "qWhfjvTdkYrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 ü§ñ Unsupervised Learning ‚Äî Explained Simply\n",
        "\n",
        "Unsupervised learning is a type of machine learning where the model **finds patterns** in data **without any labels**. It's like exploring a new place with no guide ‚Äî the model figures out the structure on its own.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç How It Works\n",
        "\n",
        "1. **Input Data (X)**: Only features, no correct answers\n",
        "2. **Model**: Learns hidden patterns, groupings, or structures in the data\n",
        "3. **Goal**: Discover insights or simplify the data\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Goals (Tasks)\n",
        "\n",
        "### 1. Clustering\n",
        "- Group similar items together\n",
        "- Example: Group customers by shopping behavior\n",
        "\n",
        "### 2. Dimensionality Reduction\n",
        "- Reduce the number of features while keeping key information\n",
        "- Example: Visualizing high-dimensional data in 2D\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Common Algorithms\n",
        "\n",
        "| Task                   | Algorithms                            |\n",
        "|------------------------|----------------------------------------|\n",
        "| Clustering             | K-Means, DBSCAN, Hierarchical Clustering |\n",
        "| Dimensionality Reduction | PCA, t-SNE, Autoencoders               |\n",
        "| Association Rule Mining | Apriori, Eclat                        |\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Process\n",
        "\n",
        "1. **Collect Data** (no labels needed)\n",
        "2. **Preprocess** (clean, scale, etc.)\n",
        "3. **Train** the model to find patterns\n",
        "4. **Analyze** output (e.g. clusters, components)\n",
        "5. **Interpret** results for decision-making\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Pros\n",
        "- No need for labeled data\n",
        "- Good for exploring unknown patterns\n",
        "\n",
        "## ‚ùå Cons\n",
        "- Harder to evaluate results\n",
        "- Can find meaningless patterns if not used carefully\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Summary\n",
        "> Unsupervised learning helps machines understand the hidden structure of data ‚Äî great for clustering, visualization, and data exploration when you don't have labels.\n"
      ],
      "metadata": {
        "id": "3V00xriQuKIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 # üß† Machine Learning & Bayes' Theorem\n",
        "\n",
        "Bayes‚Äô Theorem is a way to **update our beliefs** (probabilities) based on **new evidence**. It‚Äôs used in **probabilistic models** in Machine Learning.\n",
        "\n",
        "---\n",
        "\n",
        "## üìò Formula\n",
        "\n",
        "**Bayes' Theorem:**\n",
        "$$\n",
        "\\[\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "\\]\n",
        "$$\n",
        "---\n",
        "\n",
        "## üß© Key Terms\n",
        "\n",
        "- **Prior (P(A))**: What we believe before seeing data\n",
        "- **Likelihood (P(B|A))**: Probability of seeing data if A is true\n",
        "- **Posterior (P(A|B))**: Updated belief after seeing data\n",
        "- **Evidence (P(B))**: Total probability of the data\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ In ML\n",
        "\n",
        "Used in:\n",
        "- **Naive Bayes classifier**\n",
        "- **Bayesian inference**\n",
        "- **Bayesian neural networks**\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Example (Spam Detection)\n",
        "\n",
        "- **Prior**: Chance an email is spam (e.g., 20%)\n",
        "- **Likelihood**: If \"free\" appears, how likely is spam?\n",
        "- **Posterior**: New spam probability after seeing \"free\"\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Summary\n",
        "\n",
        "> Bayes‚Äô theorem lets ML models **learn from evidence** by updating probabilities. It‚Äôs the core of **Bayesian thinking** in AI.\n"
      ],
      "metadata": {
        "id": "lXdAlUnLud_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 # üéØ Error Decomposition: Bias-Variance Tradeoff\n",
        "\n",
        "In Machine Learning, total prediction error can be split into three parts: **bias**, **variance**, and **irreducible error**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Total Error = Bias¬≤ + Variance + Irreducible Error\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Bias\n",
        "- **What**: Error from wrong assumptions in the model.\n",
        "- **High Bias**: Model is too simple ‚Üí underfitting.\n",
        "- **Example**: Linear model for complex patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Variance\n",
        "- **What**: Error from model sensitivity to training data.\n",
        "- **High Variance**: Model is too complex ‚Üí overfitting.\n",
        "- **Example**: Model learns noise in training data.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öñÔ∏è Tradeoff\n",
        "- **Goal**: Find the balance between bias and variance.\n",
        "- **Simple model** ‚Üí low variance, high bias.\n",
        "- **Complex model** ‚Üí low bias, high variance.\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Visualization\n",
        "\n",
        "- Underfitting: üéØ misses target completely ‚Üí high bias\n",
        "- Overfitting: üéØ hits different spots every time ‚Üí high variance\n",
        "- Good fit: üéØ hits near the center consistently\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary\n",
        "\n",
        "> Bias-Variance tradeoff explains **why models make errors** and helps us choose **right model complexity**.\n"
      ],
      "metadata": {
        "id": "_lPvlkZFvjWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 üìâ Linear Regression vs üîê Logistic Regression\n",
        "\n",
        "---\n",
        "\n",
        "## üî¢ Linear Regression\n",
        "\n",
        "- **Goal**: Predict a continuous value (e.g., price, weight).\n",
        "- **Formula**:  \n",
        "  `y = w‚ÇÄ + w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô`  \n",
        "  (a straight line or hyperplane)\n",
        "\n",
        "- **How it works**:  \n",
        "  Finds the best line that fits the training data by minimizing the difference between predicted and actual values (using **Mean Squared Error**).\n",
        "\n",
        "- **Example**:  \n",
        "  Predict house price based on area and number of rooms.\n",
        "\n",
        "---\n",
        "\n",
        "## üö¶ Logistic Regression\n",
        "\n",
        "- **Goal**: Predict probability for **classification** (e.g., spam or not spam).\n",
        "- **Formula**:  \n",
        "  `P(y=1|x) = 1 / (1 + e^-(w‚ÇÄ + w‚ÇÅx‚ÇÅ + ... + w‚Çôx‚Çô))`  \n",
        "  (sigmoid function)\n",
        "\n",
        "- **How it works**:  \n",
        "  Outputs a number between 0 and 1 ‚Üí interprets as probability ‚Üí sets threshold (e.g., 0.5) for classification.\n",
        "\n",
        "- **Example**:  \n",
        "  Predict if a student passes (yes/no) based on study hours.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary Table\n",
        "\n",
        "| Feature               | Linear Regression         | Logistic Regression          |\n",
        "|----------------------|---------------------------|------------------------------|\n",
        "| Output Type          | Continuous number          | Probability / Class (0 or 1) |\n",
        "| Used For             | Regression problems        | Binary classification        |\n",
        "| Activation Function  | None (just linear)         | Sigmoid                      |\n",
        "| Loss Function        | Mean Squared Error (MSE)   | Log Loss (Cross Entropy)     |\n"
      ],
      "metadata": {
        "id": "q3-GDbB2vlP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 # ü§ñ k-NN, SVM, and Kernel Trick\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ k-Nearest Neighbors (k-NN)\n",
        "\n",
        "- **What it is**: A lazy, simple algorithm that classifies a point by looking at its **k closest neighbors**.\n",
        "- **How it works**:\n",
        "  1. Choose `k` (e.g., 3).\n",
        "  2. Measure distance (usually Euclidean) to all points in training set.\n",
        "  3. Find the `k` nearest ones.\n",
        "  4. Assign the most common label among them.\n",
        "\n",
        "- **Use**: Classification or regression.\n",
        "\n",
        "- **Example**: To classify a fruit by shape/size, check the 3 most similar fruits already labeled.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è Support Vector Machines (SVM)\n",
        "\n",
        "- **What it is**: A powerful classifier that finds the **best boundary (hyperplane)** that separates classes.\n",
        "- **Goal**: Maximize the **margin** between two classes.\n",
        "\n",
        "- **How it works**:\n",
        "  - Finds a hyperplane that best separates the data.\n",
        "  - Only the closest points (called **support vectors**) affect the boundary.\n",
        "\n",
        "- **Use**: Works well for high-dimensional data and text classification.\n",
        "\n",
        "- **Example**: Email spam detection, image classification.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Kernel Trick\n",
        "\n",
        "- **Problem**: Some data is **not linearly separable** (can‚Äôt draw a straight line to split).\n",
        "- **Solution**: Use a **kernel** function to map data into a **higher dimension** where it becomes separable.\n",
        "\n",
        "- **Popular Kernels**:\n",
        "  - Polynomial\n",
        "  - Radial Basis Function (RBF)\n",
        "\n",
        "- **Key idea**: Do math to simulate higher dimensions **without actually computing them** (saves time and memory).\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary\n",
        "\n",
        "| Method       | Type         | Strength                          | Weakness                      |\n",
        "|--------------|--------------|-----------------------------------|-------------------------------|\n",
        "| k-NN         | Lazy learner | Easy to understand and use        | Slow for large datasets       |\n",
        "| SVM          | Hard margin  | Works well with high-dimensional data | Not good for very large datasets |\n",
        "| Kernel Trick |\n"
      ],
      "metadata": {
        "id": "rZW09Owsv5gF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 # üå≥ Decision Trees and How They Work\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ What is a Decision Tree?\n",
        "\n",
        "A **decision tree** is a flowchart-like model used for **classification** or **regression**.  \n",
        "It splits data into **branches** based on features, leading to a final **decision (leaf)**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß± How It Works\n",
        "\n",
        "1. **Start** at the root (all data).\n",
        "2. **Choose the best feature** to split the data (based on criteria like Gini or Entropy).\n",
        "3. **Split** the data into groups.\n",
        "4. Repeat for each branch until:\n",
        "   - All data in a node is pure (same class), or\n",
        "   - Max depth or min samples is reached.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Splitting Criteria\n",
        "\n",
        "- **Gini Impurity** (used in CART): Measures how mixed the labels are.\n",
        "- **Entropy & Information Gain** (used in ID3/C4.5):\n",
        "  - Entropy: Disorder in the data.\n",
        "  - Info Gain: How much disorder is reduced by the split.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Example\n",
        "\n",
        "If predicting if a person will buy a phone:\n",
        "- Root: Age\n",
        "  - Age < 30 ‚Üí Student?\n",
        "    - Yes ‚Üí Buys\n",
        "    - No ‚Üí Doesn‚Äôt buy\n",
        "  - Age > 30 ‚Üí Income?\n",
        "    - High ‚Üí Buys\n",
        "    - Low ‚Üí Doesn‚Äôt buy\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è Pros and Cons\n",
        "\n",
        "| Pros                        | Cons                             |\n",
        "|-----------------------------|----------------------------------|\n",
        "| Easy to understand and use  | Can overfit (too specific)       |\n",
        "| Works with numeric/categorical data | Not great with noisy data   |\n",
        "| No need to normalize data   | Instable to small changes        |\n",
        "\n",
        "---\n",
        "\n",
        "## üå≤ Final Tip\n",
        "\n",
        "For better performance, use **Random Forests** (many trees combined) to reduce overfitting.\n"
      ],
      "metadata": {
        "id": "IPccMyDXwDnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 # üî¢ Features in Machine Learning\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Types of Features\n",
        "\n",
        "1. **Numerical (Continuous)**  \n",
        "   - Example: Age, Salary  \n",
        "   - Can be any number.\n",
        "\n",
        "2. **Categorical (Discrete)**  \n",
        "   - Example: Gender, Country  \n",
        "   - Stored as labels or strings.\n",
        "\n",
        "3. **Ordinal**  \n",
        "   - Ordered categories.  \n",
        "   - Example: Education level (High School < Bachelor < Master).\n",
        "\n",
        "4. **Boolean/Binary**  \n",
        "   - True/False, 0/1  \n",
        "   - Example: IsStudent = Yes/No\n",
        "\n",
        "5. **Text / Time / Image / Audio**  \n",
        "   - Require special preprocessing.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Feature Selection Approaches\n",
        "\n",
        "Used to pick only the **most useful features** (avoid noise and reduce overfitting):\n",
        "\n",
        "1. **Filter Methods**  \n",
        "   - Use statistics like correlation, chi-squared.  \n",
        "   - Fast but ignore model performance.\n",
        "\n",
        "2. **Wrapper Methods**  \n",
        "   - Use model performance to evaluate combinations (e.g., forward/backward selection).  \n",
        "   - More accurate but slower.\n",
        "\n",
        "3. **Embedded Methods**  \n",
        "   - Feature selection built into the model (e.g., Lasso Regression, Decision Trees).\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ One-Hot Encoding\n",
        "\n",
        "A method to convert **categorical features** into numbers:\n",
        "\n",
        "- Creates a new column for each unique value.\n",
        "- Puts `1` where it matches, `0` elsewhere.\n",
        "\n",
        "### Example:\n",
        "\n",
        "| Country   | ‚Üí One-Hot |\n",
        "|-----------|-----------|\n",
        "| France    | [1, 0, 0] |\n",
        "| Germany   | [0, 1, 0] |\n",
        "| Spain     | [0, 0, 1] |\n",
        "\n",
        "Used to make **categorical data usable** by machine learning models.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary\n",
        "\n",
        "- Different feature types need different handling.\n",
        "- Feature selection improves performance.\n",
        "- One-hot encoding transforms categories into machine-friendly format.\n"
      ],
      "metadata": {
        "id": "vCpvHWbaw7zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 # üìä Data Visualization & Dimensionality Reduction\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Data Visualization\n",
        "\n",
        "**Data visualization** is the process of representing data graphically. It helps in understanding the structure, patterns, and relationships in the data. Common visualizations:\n",
        "\n",
        "- **Histograms**: Distribution of a single variable.\n",
        "- **Scatter Plots**: Relationship between two continuous variables.\n",
        "- **Bar Charts**: Comparison of categorical data.\n",
        "- **Heatmaps**: Correlation or intensity of data across dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "## üîΩ Principal Component Analysis (PCA)\n",
        "\n",
        "PCA is a **dimensionality reduction** technique that transforms data into fewer dimensions while retaining as much information as possible.\n",
        "\n",
        "### How It Works:\n",
        "- Identifies the **principal components** (directions in which data varies the most).\n",
        "- Projects the data onto these components.\n",
        "- Reduces dimensions by keeping only the most important components.\n",
        "\n",
        "### Example:\n",
        "For a dataset with many variables (features), PCA reduces it to 2-3 main features that still represent the data well, making it easier to visualize or analyze.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
        "\n",
        "t-SNE is a **non-linear dimensionality reduction** technique designed for **visualizing high-dimensional data** in 2 or 3 dimensions. It focuses on preserving local structures.\n",
        "\n",
        "### How It Works:\n",
        "- It minimizes the difference between pairwise similarities in high and low-dimensional space.\n",
        "- Helps reveal patterns like clusters or groups in data.\n",
        "\n",
        "### When to Use:\n",
        "- Best used for visualizing data like images, text embeddings, or anything with high-dimensional features.\n",
        "\n",
        "### Example:\n",
        "It‚Äôs often used to visualize the clustering of data, like when you apply it to a neural network's activations or a word embedding.\n",
        "\n",
        "---\n",
        "\n",
        "## üìâ Summary\n",
        "\n",
        "- **Data Visualization** helps to understand data visually.\n",
        "- **PCA** is a linear method that reduces the dimensionality while retaining the variance.\n",
        "- **t-SNE** is a non-linear method focused on preserving local relationships for better visualization.\n",
        "\n"
      ],
      "metadata": {
        "id": "OGTShPWBxAok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 # üßÆ Classification Metrics & Evaluation\n",
        "\n",
        "---\n",
        "\n",
        "## üî¢ Key Classification Metrics\n",
        "\n",
        "### 1. **Accuracy**\n",
        "- **Definition**: The proportion of correctly predicted instances out of the total instances.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{Accuracy} = \\frac{\\text{True Positives + True Negatives}}{\\text{Total Instances}}\n",
        "  $$\n",
        "- **Use Case**: Good for balanced datasets but not ideal for imbalanced datasets.\n",
        "\n",
        "### 2. **Precision**\n",
        "- **Definition**: The proportion of true positive predictions out of all positive predictions made.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\n",
        "  $$\n",
        "- **Use Case**: Useful when false positives are more costly (e.g., email spam detection).\n",
        "\n",
        "### 3. **Recall (Sensitivity or True Positive Rate)**\n",
        "- **Definition**: The proportion of true positive predictions out of all actual positives.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\n",
        "  $$\n",
        "- **Use Case**: Useful when missing a positive instance is costly (e.g., detecting diseases).\n",
        "\n",
        "### 4. **F1-Score**\n",
        "- **Definition**: The harmonic mean of precision and recall, balancing both metrics.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}\n",
        "  $$\n",
        "- **Use Case**: Useful when both false positives and false negatives are important and you need a balanced metric.\n",
        "\n",
        "### 5. **Log-Loss (Logarithmic Loss)**\n",
        "- **Definition**: Measures the accuracy of a classifier by penalizing incorrect classifications, especially when confident about wrong predictions.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{Log-Loss} = - \\frac{1}{N} \\sum_{i=1}^N y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)\n",
        "  $$\n",
        "  where \\( y_i \\) is the true label and \\( p_i \\) is the predicted probability.\n",
        "- **Use Case**: Especially useful for probabilistic classifiers (e.g., logistic regression, neural networks).\n",
        "\n",
        "### 6. **ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**\n",
        "- **Definition**: A performance measurement for classification problems at various thresholds settings.\n",
        "- **Use Case**: Evaluates the trade-off between true positive rate and false positive rate. The higher the AUC, the better the model.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Confusion Matrix\n",
        "\n",
        "A **Confusion Matrix** is a table that describes the performance of a classification model by comparing predicted labels with true labels. It contains four values:\n",
        "\n",
        "|               | Predicted Positive | Predicted Negative |\n",
        "|---------------|--------------------|--------------------|\n",
        "| **Actual Positive**   | True Positive (TP)  | False Negative (FN)  |\n",
        "| **Actual Negative**   | False Positive (FP) | True Negative (TN)  |\n",
        "\n",
        "- **True Positive (TP)**: Correctly predicted positive class.\n",
        "- **False Positive (FP)**: Incorrectly predicted as positive.\n",
        "- **True Negative (TN)**: Correctly predicted negative class.\n",
        "- **False Negative (FN)**: Incorrectly predicted as negative.\n",
        "\n",
        "### **Types of Errors**\n",
        "- **False Positive (Type I error)**: Incorrectly predicting a positive when it‚Äôs actually negative.\n",
        "- **False Negative (Type II error)**: Incorrectly predicting a negative when it‚Äôs actually positive.\n",
        "\n",
        "---\n",
        "\n",
        "## üìè Metrics for Regression Models\n",
        "\n",
        "### 1. **Mean Absolute Error (MAE)**\n",
        "- **Definition**: The average of the absolute errors between predicted and actual values.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{MAE} = \\frac{1}{N} \\sum_{i=1}^N |y_i - \\hat{y}_i|\n",
        "  $$\n",
        "\n",
        "### 2. **Mean Squared Error (MSE)**\n",
        "- **Definition**: The average of the squared differences between predicted and actual values. More sensitive to large errors.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n",
        "  $$\n",
        "\n",
        "### 3. **Root Mean Squared Error (RMSE)**\n",
        "- **Definition**: The square root of MSE. Gives an idea of the magnitude of error in the same units as the original data.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  \\text{RMSE} = \\sqrt{\\text{MSE}}\n",
        "  $$\n",
        "\n",
        "### 4. **R-squared (R¬≤)**\n",
        "- **Definition**: Measures how well the model explains the variance of the target variable. Ranges from 0 to 1, with higher values indicating a better fit.\n",
        "- **Formula**:\n",
        "  $$\n",
        "  R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## üìâ Summary\n",
        "\n",
        "- **Classification Metrics**: Help assess the performance of classification models with metrics like accuracy, precision, recall, and F1-score.\n",
        "- **Confusion Matrix**: A useful table to visualize the performance of a classification model and the types of errors made.\n",
        "- **Regression Metrics**: Used to evaluate regression models with metrics like MAE, MSE, RMSE, and R¬≤.\n"
      ],
      "metadata": {
        "id": "lTs8LvI5xPQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11 # üå≤ Ensemble Methods & Random Forest\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ What is an Ensemble?\n",
        "\n",
        "An **ensemble** combines multiple models (called **base learners**) to improve prediction performance.\n",
        "\n",
        "### üéØ Why use ensembles?\n",
        "- Reduce **variance** (bagging)\n",
        "- Reduce **bias** (boosting)\n",
        "- Improve **generalization** over single models\n",
        "\n",
        "---\n",
        "\n",
        "## üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Types of Ensembles\n",
        "\n",
        "1. **Bagging (Bootstrap Aggregating)**\n",
        "   - Trains models on different random subsets of data (with replacement).\n",
        "   - Final prediction: average (regression) or majority vote (classification).\n",
        "   - Example: **Random Forest**.\n",
        "\n",
        "2. **Boosting**\n",
        "   - Models are trained sequentially.\n",
        "   - Each new model corrects errors made by the previous one.\n",
        "   - Examples: AdaBoost, Gradient Boosting.\n",
        "\n",
        "3. **Stacking**\n",
        "   - Combines different types of models.\n",
        "   - A **meta-model** is trained to combine outputs of base models.\n",
        "\n",
        "---\n",
        "\n",
        "## üå≥ Random Forest\n",
        "\n",
        "Random Forest is an ensemble of many **Decision Trees** trained using **bagging**.\n",
        "\n",
        "### üîß How it works:\n",
        "1. Draw **bootstrapped samples** from training data.\n",
        "2. Train a decision tree on each sample.\n",
        "3. At each split in the tree, select the **best feature** from a random subset of features.\n",
        "4. Aggregate predictions:\n",
        "   - **Classification**: majority vote.\n",
        "   - **Regression**: average.\n",
        "\n",
        "### üìà Why it works:\n",
        "- Reduces **overfitting** of individual trees.\n",
        "- Decorrelates trees using feature randomness.\n",
        "\n",
        "---\n",
        "\n",
        "## üî¢ Random Forest Prediction\n",
        "\n",
        "For classification:\n",
        "$$\n",
        "\\hat{y} = \\text{majority\\_vote}(T_1(x), T_2(x), ..., T_k(x))\n",
        "$$\n",
        "\n",
        "For regression:\n",
        "$$\n",
        "\\hat{y} = \\frac{1}{k} \\sum_{i=1}^{k} T_i(x)\n",
        "$$\n",
        "\n",
        "Where \\( T_i(x) \\) is the prediction of the \\( i \\)-th tree.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Advantages\n",
        "- High accuracy\n",
        "- Works well with large datasets\n",
        "- Handles missing data\n",
        "- Less need for parameter tuning\n",
        "\n",
        "## ‚ö†Ô∏è Disadvantages\n",
        "- Slower for real-time predictions\n",
        "- Less interpretable than a single tree\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "- **Ensemble learning** = many models combined for better performance.\n",
        "- **Random Forest** = many decision trees + randomness + aggregation.\n",
        "- Useful in both classification and regression tasks.\n"
      ],
      "metadata": {
        "id": "bmCI9bVveY61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12 # üîó Clustering Algorithms: K-Means, DBSCAN & Quality Estimation\n",
        "\n",
        "---\n",
        "\n",
        "## üìä What is Clustering?\n",
        "\n",
        "**Clustering** is **unsupervised learning** that groups similar data points based on distance or density ‚Äî without labeled outputs.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è K-Means\n",
        "\n",
        "- Objective: Minimize within-cluster variance.\n",
        "- Steps:\n",
        "  1. Choose \\( k \\) cluster centers (centroids).\n",
        "  2. Assign points to the nearest centroid.\n",
        "  3. Update centroids as mean of assigned points.\n",
        "  4. Repeat until convergence.\n",
        "\n",
        "### Formula (objective):\n",
        "$$\n",
        "\\min_{C} \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2\n",
        "$$\n",
        "\n",
        "‚úÖ Fast, scalable  \n",
        "‚ùå Needs \\( k \\), fails on non-spherical clusters\n",
        "\n",
        "---\n",
        "\n",
        "## üß± DBSCAN (Density-Based Spatial Clustering)\n",
        "\n",
        "- Groups dense regions, separates noise.\n",
        "- Parameters:\n",
        "  - \\( \\varepsilon \\): neighborhood radius\n",
        "  - `minPts`: minimum points to form a dense region\n",
        "\n",
        "‚úÖ Finds arbitrary shapes, handles noise  \n",
        "‚ùå Struggles with varying density\n",
        "\n",
        "---\n",
        "\n",
        "## üìè Clustering Quality Estimation\n",
        "\n",
        "### 1. **Silhouette Score**  \n",
        "How close a point is to its cluster vs. other clusters:\n",
        "$$\n",
        "s = \\frac{b - a}{\\max(a, b)}\n",
        "$$\n",
        "- \\( a \\) = intra-cluster distance  \n",
        "- \\( b \\) = nearest-cluster distance  \n",
        "- \\( s \\in [-1, 1] \\), higher = better\n",
        "\n",
        "### 2. **Davies-Bouldin Index**  \n",
        "Lower is better ‚Äî measures average \"similarity\" between clusters.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ Clustering helps find hidden structures in data without labels.\n"
      ],
      "metadata": {
        "id": "sRj22P9oetqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13 # üß† Multilayer Perceptron (MLP), Activation & Loss Functions\n",
        "\n",
        "---\n",
        "\n",
        "## üîó What is an MLP?\n",
        "\n",
        "**Multilayer Perceptron** is a type of **feedforward neural network** with:\n",
        "- Input layer ‚Üí Hidden layer(s) ‚Üí Output layer\n",
        "- Each neuron computes:\n",
        "$$\n",
        "z = w^T x + b, \\quad a = \\sigma(z)\n",
        "$$\n",
        "\n",
        "It learns by **backpropagation**, adjusting weights using gradients from a loss function.\n",
        "\n",
        "---\n",
        "\n",
        "# ‚ö° Activation Functions in Neural Networks\n",
        "\n",
        "Activation functions introduce **non-linearity**, allowing neural networks to learn complex patterns. Without them, a neural network would be just a linear model, regardless of how many layers it has.\n",
        "\n",
        "---\n",
        "\n",
        "| Function      | Formula                                                 | Notes                                                                 |\n",
        "|---------------|---------------------------------------------------------|-----------------------------------------------------------------------|\n",
        "| **Sigmoid**   | $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$                  | Maps any input to range (0, 1). Often used in binary classification. |\n",
        "| **Tanh**      | $$ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$      | Output is in range (-1, 1), centered at 0. Often better than sigmoid. |\n",
        "| **ReLU**      | $$ \\text{ReLU}(x) = \\max(0, x) $$                       | Very fast to compute. Common in hidden layers. Can lead to \"dead\" neurons. |\n",
        "| **Leaky ReLU**| $$ \\text{LeakyReLU}(x) = \\max(0.01x, x) $$              | Fixes dying ReLU by allowing a small slope for negative inputs.      |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† How Activation Functions Work:\n",
        "\n",
        "1. **Placed after each linear transformation** (e.g., weighted sum + bias).\n",
        "2. **Adds non-linearity**, which allows stacking layers to model complex data (like images or language).\n",
        "3. **Enables gradient-based learning** (via backpropagation).\n",
        "\n",
        "### üí° Example:\n",
        "\n",
        "In a single neuron:\n",
        "$$\n",
        "z = w^T x + b,\\quad a = \\text{Activation}(z)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( z \\) is the linear output\n",
        "- \\( a \\) is the activated output (e.g., via sigmoid or ReLU)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary:\n",
        "\n",
        "- Use **ReLU** for hidden layers (fast and effective).\n",
        "- Use **Sigmoid** or **Softmax** in the output for classification.\n",
        "- Pick activation based on your task, and always monitor performance and gradients.\n",
        "\n",
        "---\n",
        "\n",
        "## üí• Loss Functions\n",
        "\n",
        "Measure the model's prediction error:\n",
        "\n",
        "- **Regression**:\n",
        "  - MSE:  \n",
        "    $$\n",
        "    \\text{MSE} = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2\n",
        "    $$\n",
        "\n",
        "- **Classification**:\n",
        "  - Binary Cross-Entropy:  \n",
        "    $$\n",
        "    -[y \\log \\hat{y} + (1 - y)\\log(1 - \\hat{y})]\n",
        "    $$\n",
        "  - Categorical Cross-Entropy for multi-class.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Parameters vs Hyperparameters\n",
        "\n",
        "- **Parameters**: Learned during training (weights \\( w \\), biases \\( b \\))\n",
        "- **Hyperparameters**: Set before training (e.g. learning rate, layers, batch size)\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ MLPs are the base of deep learning ‚Äî combining linear algebra, activation, and optimization to learn complex patterns.\n"
      ],
      "metadata": {
        "id": "TYAmlyMmfCQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14 # üß† Training Deep Neural Networks as an Optimization Problem\n",
        "\n",
        "Training a neural network means finding parameters (weights and biases) that minimize a **loss function** (how wrong predictions are). This is framed as an **optimization problem**.\n",
        "\n",
        "## üéØ Objective:\n",
        "Minimize loss \\( L(\\theta) \\), where \\( \\theta \\) are model parameters.\n",
        "\n",
        "---\n",
        "\n",
        "## üîΩ Gradient Descent (GD)\n",
        "\n",
        "Basic algorithm that updates parameters in the direction of negative gradient:\n",
        "\n",
        "$$\n",
        "\\theta := \\theta - \\eta \\cdot \\nabla_\\theta L(\\theta)\n",
        "$$\n",
        "\n",
        "- \\( \\eta \\): learning rate\n",
        "- \\( \\nabla_\\theta L \\): gradient of the loss\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Instead of using the full dataset, updates are made per sample or small batches:\n",
        "\n",
        "$$\n",
        "\\theta := \\theta - \\eta \\cdot \\nabla_\\theta L(\\theta; x_i, y_i)\n",
        "$$\n",
        "\n",
        "‚úÖ Faster updates  \n",
        "‚ùå Noisy but helps escape local minima\n",
        "\n",
        "---\n",
        "\n",
        "## üåÄ Momentum\n",
        "\n",
        "Adds velocity to updates to smooth them:\n",
        "\n",
        "$$\n",
        "v := \\beta v - \\eta \\nabla_\\theta L(\\theta) \\\\\n",
        "\\theta := \\theta + v\n",
        "$$\n",
        "\n",
        "- \\( \\beta \\in [0,1] \\): momentum factor (e.g., 0.9)\n",
        "\n",
        "---\n",
        "\n",
        "## üìâ RMSProp\n",
        "\n",
        "Adapts learning rate based on recent gradient magnitudes:\n",
        "\n",
        "$$\n",
        "s := \\rho s + (1 - \\rho) \\cdot (\\nabla_\\theta L(\\theta))^2 \\\\\n",
        "\\theta := \\theta - \\frac{\\eta}{\\sqrt{s + \\epsilon}} \\cdot \\nabla_\\theta L(\\theta)\n",
        "$$\n",
        "\n",
        "- \\( \\rho \\): decay rate (e.g., 0.9)\n",
        "- Helps deal with varying gradients\n",
        "\n",
        "---\n",
        "\n",
        "## üß¨ Adam (Adaptive Moment Estimation)\n",
        "\n",
        "Combines **Momentum** and **RMSProp**:\n",
        "\n",
        "$$\n",
        "m_t := \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_\\theta L(\\theta) \\\\\n",
        "v_t := \\beta_2 v_{t-1} + (1 - \\beta_2)(\\nabla_\\theta L(\\theta))^2 \\\\\n",
        "\\hat{m}_t := \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t := \\frac{v_t}{1 - \\beta_2^t} \\\\\n",
        "\\theta := \\theta - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t\n",
        "$$\n",
        "\n",
        "- \\( \\beta_1 = 0.9 \\), \\( \\beta_2 = 0.999 \\): common defaults  \n",
        "- Best choice for most deep learning tasks\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "| Algorithm | Pros | Cons |\n",
        "|----------|------|------|\n",
        "| GD | Stable, precise | Slow for big data |\n",
        "| SGD | Fast updates | Noisy |\n",
        "| Momentum | Faster convergence | Needs tuning |\n",
        "| RMSProp | Adapts LR | May overshoot |\n",
        "| Adam | Best of all | Slightly more complex |\n",
        "\n"
      ],
      "metadata": {
        "id": "-qz9aDAgfeo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15 # ü§ñ Deep Multi-Layer Neural Networks\n",
        "\n",
        "A **deep neural network (DNN)** is a model with multiple layers of neurons between input and output. Each layer learns features from the previous one.\n",
        "\n",
        "## üîÅ Backpropagation Algorithm\n",
        "\n",
        "Backpropagation is used to compute gradients of the loss function w.r.t. each parameter by applying the chain rule from output to input layer.\n",
        "\n",
        "For a given layer \\( l \\):\n",
        "\n",
        "$$\n",
        "\\delta^l = \\frac{\\partial L}{\\partial z^l} = (\\delta^{l+1} \\cdot W^{l+1}) \\odot f'(z^l)\n",
        "$$\n",
        "\n",
        "- \\( \\delta^l \\): error at layer \\( l \\)  \n",
        "- \\( W^{l+1} \\): weights of next layer  \n",
        "- \\( f'(z^l) \\): derivative of activation function  \n",
        "- \\( \\odot \\): element-wise multiplication\n",
        "\n",
        "Weights are updated as:\n",
        "\n",
        "$$\n",
        "W^l := W^l - \\eta \\cdot \\frac{\\partial L}{\\partial W^l}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Vanishing and Exploding Gradients\n",
        "\n",
        "In deep networks, gradients can:\n",
        "\n",
        "- **Vanishing**: become very small ‚Üí slow learning\n",
        "- **Exploding**: become very large ‚Üí unstable updates\n",
        "\n",
        "### Why?\n",
        "\n",
        "When multiplying many derivatives (chain rule), if they're <1 ‚Üí shrink (vanish); if >1 ‚Üí grow (explode).\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Solutions\n",
        "\n",
        "| Problem | Solutions |\n",
        "|--------|-----------|\n",
        "| Vanishing | ‚úÖ ReLU/Leaky ReLU activations (no small gradients)  \n",
        "|          | ‚úÖ Batch normalization  \n",
        "|          | ‚úÖ Proper weight initialization (e.g. He, Xavier)  \n",
        "|          | ‚úÖ Use residual connections (ResNets)  \n",
        "| Exploding | ‚úÖ Gradient clipping  \n",
        "|           | ‚úÖ Lower learning rates |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "- Deep networks extract complex features.\n",
        "- Backprop is key for training.\n",
        "- Vanishing/exploding gradients slow or destabilize training.\n",
        "- Use good activations, normalization, and initialization to fix.\n"
      ],
      "metadata": {
        "id": "b12TWkInfkmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16 # üìä Datasets and Overfitting in Machine Learning\n",
        "\n",
        "## üß© Types of Datasets\n",
        "\n",
        "| Type       | Purpose                              |\n",
        "|------------|--------------------------------------|\n",
        "| **Train set**   | Used to train the model (learn patterns) |\n",
        "| **Validation (Dev) set** | Used to tune hyperparameters and monitor performance |\n",
        "| **Test set**    | Used to evaluate final model performance on unseen data |\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Cross-Validation\n",
        "\n",
        "A method to reliably evaluate model performance:\n",
        "\n",
        "**K-Fold Cross-Validation:**\n",
        "- Data is split into \\( k \\) parts.\n",
        "- Train on \\( k-1 \\), validate on 1 fold.\n",
        "- Repeat \\( k \\) times and average the results.\n",
        "\n",
        "### Formula for cross-validated score:\n",
        "\n",
        "$$\n",
        "\\text{CV Score} = \\frac{1}{k} \\sum_{i=1}^{k} \\text{score}_i\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Monitoring the Learning Process\n",
        "\n",
        "- **Training loss ‚Üì**: model learns on training data  \n",
        "- **Validation loss**: helps detect overfitting  \n",
        "  - If it goes **‚Üë while train loss ‚Üì**, overfitting is happening\n",
        "\n",
        "Use **learning curves** (train vs. validation loss) to visualize.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Overfitting\n",
        "\n",
        "When a model learns the training data too well, including noise or irrelevant details, and performs poorly on new data.\n",
        "\n",
        "| Symptom             | Cause                          | Solution                  |\n",
        "|---------------------|--------------------------------|---------------------------|\n",
        "| High train accuracy, low test accuracy | Model memorizes data     | Use simpler model, more data, regularization |\n",
        "\n",
        "---\n",
        "\n",
        "### Techniques to Reduce Overfitting\n",
        "\n",
        "- Cross-validation  \n",
        "- Dropout (for neural networks)  \n",
        "- Regularization (L1/L2)  \n",
        "- Early stopping  \n",
        "- Data augmentation\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "- Always split your data into **train/dev/test**.\n",
        "- Use **cross-validation** for stable results.\n",
        "- Monitor loss curves to spot **overfitting** early.\n"
      ],
      "metadata": {
        "id": "pxRnjj67fogG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17 # üß† Convolutional Neural Networks (CNNs)\n",
        "\n",
        "CNNs are powerful for processing **grid-like data**, such as **images**. They automatically learn features like edges, shapes, and patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Key Components\n",
        "\n",
        "### 1. **Convolution**\n",
        "\n",
        "A mathematical operation to extract features using filters (kernels):\n",
        "\n",
        "$$\n",
        "S(i, j) = (X * K)(i, j) = \\sum_m \\sum_n X(i+m, j+n) \\cdot K(m, n)\n",
        "$$\n",
        "\n",
        "- \\( X \\): input image  \n",
        "- \\( K \\): kernel (filter)  \n",
        "- \\( S \\): feature map (result)  \n",
        "- Learns **edges**, **textures**, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Feature Maps**\n",
        "\n",
        "- The output after convolution.\n",
        "- Highlight specific patterns.\n",
        "- Each filter creates one feature map.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Padding**\n",
        "\n",
        "Adds borders (usually zeros) to keep output size:\n",
        "\n",
        "- **Same Padding**: output size = input size  \n",
        "- **Valid Padding**: no padding, output shrinks\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Pooling**\n",
        "\n",
        "Reduces size of feature maps (downsampling):\n",
        "\n",
        "#### Max Pooling Example:\n",
        "$$\n",
        "\\text{MaxPool}(2x2): \\max \\left\\{ x_1, x_2, x_3, x_4 \\right\\}\n",
        "$$\n",
        "\n",
        "- Helps with **translation invariance**\n",
        "- Makes the network faster and less likely to overfit\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Low-Level vs High-Level Features**\n",
        "\n",
        "| Layer | Features Learned        |\n",
        "|-------|--------------------------|\n",
        "| Early (1st-2nd) | Edges, corners, colors         |\n",
        "| Middle          | Textures, patterns             |\n",
        "| Deep            | Shapes, objects, semantics     |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† CNN Architecture (Simplified)\n",
        "[Input Image]\n",
        "\n",
        "‚Üì\n",
        "\n",
        "[Convolution + ReLU]\n",
        "\n",
        "‚Üì\n",
        "\n",
        "[Pooling]\n",
        "\n",
        "‚Üì\n",
        "\n",
        "[Convolution + ReLU]\n",
        "\n",
        "‚Üì\n",
        "\n",
        "[Pooling]\n",
        "\n",
        "‚Üì\n",
        "\n",
        "[Fully Connected Layer]\n",
        "\n",
        "‚Üì\n",
        "\n",
        "[Output (e.g., class)]\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "CNNs detect **spatial patterns** in data using:\n",
        "- **Convolution** to extract features\n",
        "- **Pooling** to reduce size\n",
        "- **Padding** to control output shape\n",
        "- Gradually transform raw pixels ‚Üí meaningful objects\n"
      ],
      "metadata": {
        "id": "objQs1lcfvSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 18 # üîÑ Transfer Learning & CNN Architectures\n",
        "\n",
        "## üîÅ What is Transfer Learning?\n",
        "\n",
        "Transfer learning is using a **pre-trained model** (trained on a large dataset like ImageNet) and **fine-tuning** it for a **new, smaller task**.\n",
        "\n",
        "### Why?\n",
        "- Saves time and compute\n",
        "- Helps when you don‚Äôt have much data\n",
        "- Leverages **learned low/high-level features**\n",
        "\n",
        "---\n",
        "\n",
        "## üß† How It Works\n",
        "\n",
        "1. **Load pre-trained model** (e.g., ResNet trained on ImageNet)\n",
        "2. **Freeze early layers** (generic features like edges)\n",
        "3. **Replace last layers** with your custom task (e.g., 10-class classifier)\n",
        "4. **Fine-tune** only the new layers (or entire model)\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Modern CNN Architectures (Overview)\n",
        "\n",
        "| Model        | Key Features                                       |\n",
        "|--------------|----------------------------------------------------|\n",
        "| LeNet-5      | One of the first CNNs (used for digit recognition) |\n",
        "| AlexNet      | Revived deep CNNs; introduced ReLU, dropout        |\n",
        "| VGG16/19     | Deep, simple; only 3x3 convolutions                |\n",
        "| GoogLeNet    | Inception modules for multi-scale learning         |\n",
        "| ResNet       | **Residual blocks** to solve vanishing gradients   |\n",
        "| DenseNet     | Connects each layer to all others (feature reuse)  |\n",
        "| EfficientNet | Balances depth, width, and resolution              |\n",
        "\n",
        "---\n",
        "\n",
        "## üìÇ Open-Source Datasets for CNNs\n",
        "\n",
        "| Dataset       | Description                               |\n",
        "|---------------|-------------------------------------------|\n",
        "| **MNIST**     | Handwritten digits (28x28)                |\n",
        "| **CIFAR-10/100** | Tiny images of objects                  |\n",
        "| **ImageNet**  | 14M images, 1000 classes (huge benchmark) |\n",
        "| **COCO**      | Object detection, segmentation            |\n",
        "| **CelebA**    | Celebrity faces with attributes           |\n",
        "| **Fashion-MNIST** | Clothing items (alternative to MNIST) |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Advantages of Modern CNNs\n",
        "\n",
        "- Extract **hierarchical features** automatically\n",
        "- **Transferable** to other tasks via pretraining\n",
        "- Works well with large datasets and complex inputs (e.g., images, video)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùå Disadvantages\n",
        "\n",
        "- **Data-hungry** (without pretraining)\n",
        "- High **computational cost** for training\n",
        "- **Hard to interpret** internal decisions (black box)\n",
        "- May **overfit** on small datasets\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Summary\n",
        "\n",
        "Transfer learning + modern CNNs = powerful, efficient image models.  \n",
        "Choose an architecture based on task size, resources, and accuracy needs.\n"
      ],
      "metadata": {
        "id": "0owdZb9ZgArz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 19 # üó£ Natural Language Processing (NLP)\n",
        "\n",
        "## üß∞ Basic Concepts\n",
        "\n",
        "NLP = teaching machines to understand human language (text/speech).\n",
        "\n",
        "---\n",
        "\n",
        "## üß± 1. Bag of Words (BoW)\n",
        "\n",
        "- Represents text as **word frequency vectors** (ignores grammar & order).\n",
        "- Example:\n",
        "\n",
        "| Sentence           | \"I love cats\" | \"I love dogs\" |\n",
        "|--------------------|---------------|----------------|\n",
        "| Word Vector        | [1, 1, 1, 0]   | [1, 1, 0, 1]   |\n",
        "| Vocabulary:        | I, love, cats, dogs |\n",
        "\n",
        "‚úÖ Simple  \n",
        "‚ùå Doesn‚Äôt consider importance or context.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä 2. TF-IDF (Term Frequency ‚Äì Inverse Document Frequency)\n",
        "\n",
        "Weights words by **how important** they are in a document **relative to a corpus**.\n",
        "\n",
        "- **TF** = how often word appears in a document  \n",
        "  $$ TF(t, d) = \\frac{f_{t,d}}{\\sum_k f_{k,d}} $$\n",
        "\n",
        "- **IDF** = how rare the word is across all documents  \n",
        "  $$ IDF(t) = \\log \\left( \\frac{N}{df_t} \\right) $$\n",
        "\n",
        "- **TF-IDF Score** =  \n",
        "  $$ TF\\text{-}IDF(t,d) = TF(t,d) \\times IDF(t) $$\n",
        "\n",
        "‚úÖ Highlights rare but important words  \n",
        "‚ùå Still loses word order & meaning\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÇÔ∏è 3. Stemming vs Lemmatization\n",
        "\n",
        "- **Stemming** = cut words to their root (e.g., ‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù, ‚Äúrunner‚Äù ‚Üí ‚Äúrun‚Äù)\n",
        "- **Lemmatization** = uses grammar to get base word (e.g., ‚Äúwas‚Äù ‚Üí ‚Äúbe‚Äù, ‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù)\n",
        "\n",
        "‚úÖ Helps group similar words  \n",
        "‚ùå May distort meaning if done poorly\n",
        "\n",
        "---\n",
        "\n",
        "## üõë 4. Stop Words\n",
        "\n",
        "- Common words (e.g., ‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúand‚Äù) that are **often removed** in preprocessing.\n",
        "\n",
        "‚úÖ Removes noise  \n",
        "‚ùå Might need careful tuning depending on the task\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "| Concept          | Purpose                         |\n",
        "|------------------|----------------------------------|\n",
        "| BoW              | Basic text to vector             |\n",
        "| TF-IDF           | Word importance in corpus        |\n",
        "| Stemming/Lemmat. | Normalize words                  |\n",
        "| Stop Words       | Remove frequent non-useful words |\n",
        "\n",
        "NLP starts with **cleaning and converting** text into numbers ‚Äî then models can work!\n"
      ],
      "metadata": {
        "id": "IKorI0-4gHlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20 # üß† Word Embeddings & Language Models\n",
        "\n",
        "Word embeddings are **dense vector representations** of words ‚Äî capturing **meaning** based on context.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Why Not Use One-Hot?\n",
        "\n",
        "One-hot encoding ‚Üí High-dimensional, no relation between words  \n",
        "E.g., \"king\" and \"queen\" = totally different vectors\n",
        "\n",
        "‚úÖ Solution: Use **word embeddings** ‚Äî words close in meaning are close in space.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Skip-gram Model (Word2Vec)\n",
        "\n",
        "**Goal**: Predict **context words** given a **center word**.\n",
        "\n",
        "- Example:  \n",
        "  Sentence: \"The cat sat on the mat\"  \n",
        "  Center: \"cat\" ‚Üí Predict: [\"The\", \"sat\", \"on\"]\n",
        "\n",
        "- Objective: Maximize probability of context given the center word.\n",
        "\n",
        "$$\n",
        "\\max \\prod_{t=1}^{T} \\prod_{-c \\le j \\le c, j \\ne 0} P(w_{t+j} | w_t)\n",
        "$$\n",
        "\n",
        "- Uses **neural network** to learn embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "## üß∞ Word2Vec (Mikolov, 2013)\n",
        "\n",
        "- **Skip-gram** or **CBOW** (predict center from context)\n",
        "- Learns ~300-dim vectors\n",
        "- Fast, simple, but static (same vector for all contexts)\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ù GloVe (Global Vectors)\n",
        "\n",
        "- Uses **word co-occurrence** statistics\n",
        "- Embeddings trained from **global matrix of word counts**\n",
        "- Objective: words that co-occur frequently ‚Üí similar vectors\n",
        "\n",
        "Equation (simplified):\n",
        "\n",
        "$$\n",
        "J = \\sum_{i,j=1}^V f(X_{ij}) \\left( w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\right)^2\n",
        "$$\n",
        "\n",
        "‚úÖ Captures global relationships  \n",
        "‚ùå Still static\n",
        "\n",
        "---\n",
        "\n",
        "## üß† BERT (Bidirectional Encoder Representations from Transformers)\n",
        "\n",
        "- Uses **Transformer** model\n",
        "- Learns embeddings from **left and right context**\n",
        "- Word meaning changes **based on sentence**\n",
        "\n",
        "**Example**:\n",
        "- \"bank\" in ‚Äúriver bank‚Äù ‚â† \"bank\" in ‚Äúmoney bank‚Äù\n",
        "\n",
        "‚úÖ Contextualized  \n",
        "‚úÖ Pre-trained on huge corpora  \n",
        "‚ùå Heavy, slower to train/use\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary Table\n",
        "\n",
        "| Model     | Type        | Context-Aware | Notes                      |\n",
        "|-----------|-------------|----------------|-----------------------------|\n",
        "| Word2Vec  | Local       | ‚ùå              | Fast, static                |\n",
        "| GloVe     | Global      | ‚ùå              | Uses word co-occurrence     |\n",
        "| BERT      | Deep/Context| ‚úÖ              | Deep, bidirectional, slow   |\n",
        "\n",
        "Word embeddings help models understand **semantics** ‚Äî a key part of modern NLP. üöÄ\n"
      ],
      "metadata": {
        "id": "kYLd-6bHgRAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 21 # üîÅ Sequence Analysis Tasks & Simple RNN\n",
        "\n",
        "## üìå What is Sequence Analysis?\n",
        "\n",
        "Sequence analysis deals with **ordered data**, where **order matters** (unlike regular data).\n",
        "\n",
        "### üîç Common Tasks:\n",
        "- üìú **Text generation** (e.g., next word prediction)\n",
        "- üé∂ **Music modeling**\n",
        "- üó£Ô∏è **Speech recognition**\n",
        "- üìà **Time-series forecasting**\n",
        "- üëÄ **Video frame prediction**\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Simple RNN Architecture\n",
        "\n",
        "A **Recurrent Neural Network (RNN)** processes **one step at a time**, remembering past info using a **hidden state**.\n",
        "\n",
        "### üß© Main Idea:\n",
        "- For each time step \\( t \\):\n",
        "  - Input: \\( x_t \\)\n",
        "  - Hidden state: \\( h_t \\)\n",
        "  - Output: \\( y_t \\)\n",
        "\n",
        "### üßÆ Equations:\n",
        "$$\n",
        "h_t = \\tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h)\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_t = W_{hy}h_t + b_y\n",
        "$$\n",
        "\n",
        "- \\( W_{xh}, W_{hh}, W_{hy} \\): weight matrices  \n",
        "- \\( b_h, b_y \\): biases  \n",
        "- \\( h_{t-1} \\): memory from previous step  \n",
        "- \\( \\tanh \\): activation function\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ How It Works:\n",
        "- Input sequence: [x‚ÇÅ, x‚ÇÇ, x‚ÇÉ, ..., x‚Çô]\n",
        "- RNN processes one by one, updating hidden state \\( h_t \\)\n",
        "- Output can be at each time step (e.g., translation) or after final step (e.g., sentiment)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Limitations:\n",
        "- Struggles with **long-term dependencies** (earlier info gets lost)\n",
        "- Suffers from **vanishing gradients**\n",
        "\n",
        "‚úÖ Later models like **LSTM** and **GRU** fix this.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "| Feature            | RNN                       |\n",
        "|--------------------|---------------------------|\n",
        "| Input              | Sequential                |\n",
        "| Memory             | Yes (hidden state)        |\n",
        "| Used for           | Text, audio, time-series  |\n",
        "| Problem            | Vanishing gradient        |\n"
      ],
      "metadata": {
        "id": "w_MR9RevhEC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 22 # üß† Memory in Neural Networks: LSTM & GRU\n",
        "\n",
        "## üß© Why Memory Matters\n",
        "- In tasks like translation or time-series, models must **\"remember\"** earlier information.\n",
        "- **RNNs** struggle with long sequences (vanishing gradients).\n",
        "- üõ†Ô∏è **LSTM** and **GRU** are improved RNNs with **memory cells** to handle long-term dependencies.\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ LSTM: Long Short-Term Memory\n",
        "\n",
        "LSTM adds **gates** to control what to remember, forget, and output.\n",
        "\n",
        "### üîë Gates:\n",
        "1. **Forget Gate**: What to discard from memory  \n",
        "   $$ f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f) $$\n",
        "2. **Input Gate**: What new info to store  \n",
        "   $$ i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i) $$\n",
        "3. **Candidate**: New memory candidate  \n",
        "   $$ \\tilde{C}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c) $$\n",
        "4. **Cell State Update**: Combine old & new  \n",
        "   $$ C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t $$\n",
        "5. **Output Gate**: What to output  \n",
        "   $$ o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o) $$\n",
        "6. **Hidden State**: Final output  \n",
        "   $$ h_t = o_t \\odot \\tanh(C_t) $$\n",
        "\n",
        "- \\( \\odot \\) means element-wise multiplication  \n",
        "- \\( \\sigma \\) is sigmoid function\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° GRU: Gated Recurrent Unit\n",
        "\n",
        "Simpler than LSTM, combines some gates.\n",
        "\n",
        "### üîë Gates:\n",
        "1. **Update Gate**: Mix of old and new memory  \n",
        "   $$ z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z) $$\n",
        "2. **Reset Gate**: How much past to forget  \n",
        "   $$ r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r) $$\n",
        "3. **Candidate Memory**: New info  \n",
        "   $$ \\tilde{h}_t = \\tanh(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h) $$\n",
        "4. **Final Output**: Combined memory  \n",
        "   $$ h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t $$\n",
        "\n",
        "---\n",
        "\n",
        "## üìä LSTM vs GRU\n",
        "\n",
        "| Feature        | LSTM                  | GRU               |\n",
        "|----------------|------------------------|-------------------|\n",
        "| Gates          | 3 (input, forget, out) | 2 (update, reset) |\n",
        "| Memory Cell    | Yes                    | No                |\n",
        "| Complexity     | Higher                 | Lower             |\n",
        "| Training Speed | Slower                 | Faster            |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "- Both LSTM and GRU solve **memory loss** in RNNs.\n",
        "- GRU is **simpler** and works well in many tasks.\n",
        "- Used in **NLP**, **speech**, **stock prediction**, etc.\n"
      ],
      "metadata": {
        "id": "MUqL2oh8hKCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 23 # üîÑ Autoencoders and Representation Learning\n",
        "\n",
        "## What is an Autoencoder?\n",
        "- A type of neural network used to **learn efficient data encoding**.\n",
        "- It **compresses** input data into a smaller representation and then **reconstructs** it back.\n",
        "- Consists of two parts:\n",
        "  - **Encoder:** Compresses input \\( x \\) into a latent code \\( z \\)\n",
        "  - **Decoder:** Reconstructs input from \\( z \\)\n",
        "\n",
        "$$\n",
        "\\text{Encoder: } z = f(x) \\\\\n",
        "\\text{Decoder: } \\hat{x} = g(z)\n",
        "$$\n",
        "\n",
        "The network is trained to minimize the difference between \\( x \\) and \\( \\hat{x} \\).\n",
        "\n",
        "---\n",
        "\n",
        "## Latent Space\n",
        "- The **compressed representation** \\( z \\) lives in a lower-dimensional space called **latent space**.\n",
        "- Captures the most important features of the input data.\n",
        "- Enables tasks like **data compression**, **denoising**, and **feature extraction**.\n",
        "\n",
        "---\n",
        "\n",
        "## Representation Learning\n",
        "- Autoencoders learn **useful representations** of data automatically.\n",
        "- These representations can improve performance in other tasks like classification or clustering.\n",
        "- Helps models understand **underlying structure** without manual feature engineering.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "- Autoencoders learn to **compress and reconstruct** data.\n",
        "- Latent space is the **compact feature space** learned by the encoder.\n",
        "- Used for **dimensionality reduction**, **anomaly detection**, and **generative modeling**.\n"
      ],
      "metadata": {
        "id": "VBknoRs9hQNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Supervised learning (an overview of the tasks and algorithms).\n",
        "2. Unsupervised learning (an overview of the tasks and algorithms).\n",
        "3. Machine learning and Bayes theorem. Prior and posterior distribution.\n",
        "4. Error decomposition. Bias and variance tradeoff.\n",
        "5. Linear regression model and Logistic regression.\n",
        "6. The method of the k-nearest neighbors. Support vector machines. Kernel trick.\n",
        "7. Decision trees and principles of its construction.\n",
        "8. Types of features. Feature selection approaches. One-hot-encoding.\n",
        "9. Data visualization and dimension-reduction algorithms: PCA and t-SNE.\n",
        "10. Classification metrics. Accuracy, precision, recall, F1-score, log-loss, ROC-AUC. Types of errors, confusion matrix. Metrics of accuracy for regression models.\n",
        "11. Construction of Ensembles of algorithms. Random Forrest.\n",
        "12. Clustering algorithms. K-means and DBSCAN. Estimation of clustering quality.\n",
        "13. Multilayer perceptron. Activation functions and loss functions in neural networks. Parameters and hyperparameters.\n",
        "14. Training of the deep neural network as an optimization problem. Gradient descent, stochastic gradient descent, Momentum, RMSProp and Adam algorithms.\n",
        "15. Deep multi-layer neural networks. Backpropagation algorithm. The problem of vanishing and exploding gradients and the methods of its solution.\n",
        "16. Datasets: train, test, validation (dev) sets. Cross-validation. Monitoring the learning process. Overfitting.\n",
        "17. Convolutional neural networks (CNNs): convolution, pooling, padding, feature maps, low-level and high-level features.\n",
        "18. Transfer learning approach. An overview of modern CNN architectures and open-source datasets. Advantages and disadvantages of modern CNNs.\n",
        "19. Natural language processing. Bag of words approach. TF-IDF method. Stemming and lemmatization. Stop words.\n",
        "20. Word embeddings. Skip-gram model. Word2vec, Glove, BERT.\n",
        "21. Sequence analysis tasks. Simple recurrent neural network architecture.\n",
        "22. LSTM and GRU cells. Memory in neural networks.\n",
        "23. Autoencoders and representation learning. Latent Space\n"
      ],
      "metadata": {
        "id": "eUYShT54uJj6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oQkqw3sSeLO2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}